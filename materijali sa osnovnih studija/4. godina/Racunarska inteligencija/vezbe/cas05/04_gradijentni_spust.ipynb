{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "exempt-concrete",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "amino-validity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# osnovni gradijentni spust\n",
    "def gradient_descent(f, gradient, x0, alpha, eps, iters):\n",
    "    x = x0\n",
    "\n",
    "    for i in range(iters):\n",
    "        x_new = x - alpha * gradient(x)\n",
    "        if abs(f(x_new) - f(x)) < eps:\n",
    "            break\n",
    "        x = x_new\n",
    "        \n",
    "    result = {}\n",
    "    result['converged'] = (i != iters)\n",
    "    result['num_iters'] = i\n",
    "    result['x'] = x_new \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "british-enough",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    # neka nam x[0] bude x, a x[1] bude y\n",
    "    return 0.5 * (x[0]**2 + 10 * x[1]**2)\n",
    "\n",
    "def gradient(x):\n",
    "    return np.array([x[0], 10 * x[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "opening-origin",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'converged': True, 'num_iters': 40, 'x': array([0.02660559, 0.        ])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x0 = np.array([2, 3])\n",
    "alpha = 0.1\n",
    "eps = 0.0001\n",
    "iters = 1000\n",
    "gradient_descent(f, gradient, x0, alpha, eps, iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "labeled-soldier",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradijent sa inercijom\n",
    "def momentum(f,gradient, x0, alpha, beta, eps, iters):\n",
    "    x = x0\n",
    "    d = 0   # inercija\n",
    "    for i in range(iters):\n",
    "        # beta je parametar koji kaze koliko vaznosti dajemo toj inerciji\n",
    "        # NOTE: inercija moze da nas povuce i na stranu suprotnu od one na koju pokazuje gradijent\n",
    "        d = beta * d + alpha * gradient(x)\n",
    "        x_new = x - d\n",
    "        if abs(f(x_new) - f(x)) < eps:\n",
    "            break\n",
    "        x = x_new\n",
    "    \n",
    "    result = {}\n",
    "    result['converged'] = (i != iters)\n",
    "    result['num_iters'] = i\n",
    "    result['x'] = x_new \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "surface-quantum",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'converged': True, 'num_iters': 107, 'x': array([-0.00663911, -0.0099656 ])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta = 0.9\n",
    "momentum(f, gradient, x0, alpha, beta, eps, iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "noted-girlfriend",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nesterovljev algoritam\n",
    "def nesterov(f,gradient, x0, alpha, beta, eps, iters):\n",
    "    x = x0\n",
    "    d = 0   \n",
    "    for i in range(iters):\n",
    "        d = beta * d + alpha * gradient(x - beta*d)    # samo se ovde razlikuje u odnosu na prosli\n",
    "        x_new = x - d\n",
    "        if abs(f(x_new) - f(x)) < eps:\n",
    "            break\n",
    "        x = x_new\n",
    "    \n",
    "    result = {}\n",
    "    result['converged'] = (i != iters)\n",
    "    result['num_iters'] = i\n",
    "    result['x'] = x_new \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "latter-cherry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'converged': True, 'num_iters': 35, 'x': array([0.01251474, 0.        ])}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta = 0.9\n",
    "nesterov(f, gradient, x0, alpha, beta, eps, iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "unique-coordination",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adam(f, gradient, x0, alpha, eps, iters, beta1, beta2, delta):\n",
    "    x = x0\n",
    "    m = 0   # prvi momenat\n",
    "    v = 0   # drugi momenat\n",
    "    for i in range(1, iters + 1):   # na vezbama je izmenio da ovo ne ide od 0 jer je imao problem\n",
    "                                    # da deli sa nulom u v_hat racunanju, ali ja taj problem nisam imao\n",
    "        grad = gradient(x)\n",
    "        m = beta1 * m + (1 - beta1) * grad\n",
    "        v = beta2 * v + (1 - beta2) * grad**2\n",
    "        \n",
    "        # ovo se radi jer momenat od gradijenta nije nepristrasna ocena (?)\n",
    "        # nisam pratio bas objasnjenje, pominjao je onaj moving average iz ekonomije\n",
    "        # ideja je da kako brojevi pristizu, tako oni stariji dosli gube na znacaju\n",
    "        # kada se raspisu prve recimo 3 iteracije za racunanje m, kad se sredi dobice se\n",
    "        # m_t = (1-beta1)*sum_i=1_do_t(beta1^(t-i) * g_i)\n",
    "        # i nisam ispratio kako je dobio odatle ovu fomrulu za ocekivanje, sve g_i smo \n",
    "        # pretpsotavili da su jednaki g_t i zato imamo i gresku na kraju\n",
    "        # E(mt) = E(g_t) * (1-beta1)*sum_i=1_do_t(beta1^(t-i) ) + neka_greska\n",
    "        # ovo:   (1-beta1)*sum_i=1_do_t(beta1^(t-i) )   po nekoj formuli koji nije naveo na casu\n",
    "        # to je jednako (1-beta1 ^ t)\n",
    "        # i na krajud a se dobije m_hat, treba m_t da podelimo sa (1-beta1 ^ t)   (nisam razumeo zasto)\n",
    "        # i to sada vise nije pristrasno ka nuli (?)\n",
    "        m_hat = m / (1 - beta1**i)\n",
    "        # slicno i za v sve\n",
    "        v_hat = v / (1 - beta2**i)\n",
    "        \n",
    "        # note: ovde v moze da bude vektor, i onda je ovo deljenje po koordinatama u numpyju\n",
    "        # delta je neki mali broj koji se dodaje za svaki slucaj da nebi podelili nulom\n",
    "        x_new = x - alpha * (m_hat / (np.sqrt(v_hat) + delta))\n",
    "                             \n",
    "        if abs(f(x_new) - f(x)) < eps:\n",
    "            break\n",
    "        \n",
    "        x = x_new\n",
    "    \n",
    "    result = {}\n",
    "    result['converged'] = (i != iters)\n",
    "    result['num_iters'] = i\n",
    "    result['x'] = x_new \n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "wrong-israeli",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'converged': True, 'num_iters': 81, 'x': array([0.00410105, 0.00184205])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ljudi koji su smislili adam algoritamprocenili su da su ovo\n",
    "# dobre polazne vrednosti za beta1 i beta2\n",
    "beta1 = 0.9\n",
    "beta2 = 0.999\n",
    "adam(f, gradient, x0, alpha, eps, iters, beta1, beta2, 1e-7)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
